{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swaekaa/llm_quant_sense/blob/master/Copy_of_moe_layer_sensitivity_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "2CnmgpCq7ZPm",
        "outputId": "df9bca9d-28d0-41ff-834b-d7f23db6f7b3"
      },
      "outputs": [],
      "source": [
        "# Layer-wise Quantization Sensitivity in Mixture-of-Experts Models\n",
        "\n",
        "## Goal\n",
        "'''Analyze the sensitivity of router (gating) layers versus expert layers in Mixture-of-Experts (MoE) models\n",
        "under simulated sub-4-bit quantization.\n",
        "\n",
        "## Hypothesis\n",
        "Quantization noise in MoE routing networks causes disproportionate performance degradation due to\n",
        "discrete expert selection instability, unlike dense transformers where degradation is continuous.\n",
        "\n",
        "## Constraints\n",
        "- Inference only\n",
        "- No fine-tuning\n",
        "- Consumer hardware'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAZGWJQL7zru"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lcetZ1S7_cS",
        "outputId": "9bea9b04-4110-47c0-9a74-13abec22f3a7"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.backends.cuda.matmul.allow_tf32 = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsZY8qTE8GrO",
        "outputId": "27353618-0db1-4a1f-f4c4-81396a7228b7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "\n",
        "TEXT_SAMPLES = 256\n",
        "texts = dataset[\"text\"][:TEXT_SAMPLES]\n",
        "texts = [t for t in texts if len(t.strip()) > 0]\n",
        "\n",
        "print(\"Total loaded texts:\", len(texts))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919,
          "referenced_widgets": [
            "9604bda4b44a458cb6453218204e2ae6",
            "c7bd2918930a475491f33de47d18d883",
            "172f75c7b0d24dde84e32be7cf19737c",
            "642cd914a60b41748e8ea5567ce29df1",
            "dbb79d5f31b94412a67d7d6331b00dcd",
            "0ff327cce29a462da90d2e70b6672a8c",
            "c9f84a95ceae45ec8d6e628f6ce0f007",
            "5d3bbde80a794a629dd775327e795d04",
            "063e002b589b4753a873363c467c9926",
            "c40d3d157440473aa5d93653e58e78b5",
            "848879175aff4856ae72f4a6308a5bc2"
          ]
        },
        "id": "dVmQQ4jb8IrP",
        "outputId": "24162f3d-5c54-4b71-a31f-7143e2413504"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"Qwen/Qwen1.5-MoE-A2.7B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13pfY-ABA0cd",
        "outputId": "257465b4-67c9-4f93-976b-5030da8295cd"
      },
      "outputs": [],
      "source": [
        "texts_small = texts[:64]   # 32–64 is ideal\n",
        "print(\"Eval samples:\", len(texts_small))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f-qmqRtAfyE",
        "outputId": "bb5509a7-2b6b-4eb9-d1c7-65901c6e2f24"
      },
      "outputs": [],
      "source": [
        "print(\"texts exists:\", \"texts\" in globals())\n",
        "print(\"texts_small exists:\", \"texts_small\" in globals())\n",
        "print(\"model exists:\", \"model\" in globals())\n",
        "print(\"baseline_nll exists:\", \"baseline_nll\" in globals())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xRD2mhRBzbh"
      },
      "outputs": [],
      "source": [
        "eval_prompts = [\n",
        "    \"The capital of France is\",\n",
        "    \"Machine learning models are trained by\",\n",
        "    \"In physics, energy is defined as\",\n",
        "    \"The movie was absolutely\",\n",
        "    \"A good restaurant should\",\n",
        "    \"The theory of relativity states that\",\n",
        "    \"Artificial intelligence systems can\",\n",
        "    \"The book was interesting because\",\n",
        "    \"In mathematics, a prime number is\",\n",
        "    \"The weather today is\",\n",
        "    \"The government announced that\",\n",
        "    \"Neural networks learn by adjusting\",\n",
        "    \"The experiment failed due to\",\n",
        "    \"The purpose of education is\",\n",
        "    \"The company reported earnings of\",\n",
        "    \"In biology, cells are\",\n",
        "    \"The main character decided to\",\n",
        "    \"Economic growth depends on\",\n",
        "    \"The scientist discovered that\",\n",
        "    \"The product was disappointing because\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIIH8O6jEuZC"
      },
      "outputs": [],
      "source": [
        "def compute_prompt_nll(model, prompts, max_length=32):\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for p in prompts:\n",
        "        enc = tokenizer(\n",
        "            p,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        ).to(next(model.parameters()).device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(**enc, labels=enc[\"input_ids\"])\n",
        "            total_loss += out.loss.item()\n",
        "\n",
        "    return total_loss / len(prompts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgYNyxR6EwQ5",
        "outputId": "30c00fb5-e81a-4a8d-d50d-56f2b4789b83"
      },
      "outputs": [],
      "source": [
        "baseline_nll = compute_prompt_nll(model, eval_prompts)\n",
        "print(\"Baseline prompt NLL:\", baseline_nll)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlCyDh7lExt9",
        "outputId": "9b9a3392-947f-49f3-81d4-ff17cb1d44bb"
      },
      "outputs": [],
      "source": [
        "# Inspect model structure to find router/gate layers\n",
        "for name, module in model.named_modules():\n",
        "    lname = name.lower()\n",
        "    if \"router\" in lname or \"gate\" in lname:\n",
        "        print(name, \"->\", type(module))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKcfCoxffmYV",
        "outputId": "8951ee53-1d02-4741-a5cc-09b5659ee3ee"
      },
      "outputs": [],
      "source": [
        "router_layers = []\n",
        "expert_layers = []\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "        if name.endswith(\".mlp.gate\"):\n",
        "            router_layers.append(name)\n",
        "        elif \".mlp.experts.\" in name:\n",
        "            expert_layers.append(name)\n",
        "\n",
        "print(\"Number of router layers:\", len(router_layers))\n",
        "print(\"Number of expert layers:\", len(expert_layers))\n",
        "print(\"\\nExample router layers:\")\n",
        "print(router_layers[:5])\n",
        "print(\"\\nExample expert layers:\")\n",
        "print(expert_layers[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVncW_UBsyF6"
      },
      "outputs": [],
      "source": [
        "def force_module_to_device(module, device):\n",
        "    for param in module.parameters():\n",
        "        if param.device != device:\n",
        "            param.data = param.data.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7oJx3hlRna2"
      },
      "outputs": [],
      "source": [
        "def quantize_router_safe(router_module, bits=2):\n",
        "    force_module_to_device(router_module, torch.device(\"cuda\"))\n",
        "    backup = router_module.weight.data.clone()\n",
        "    router_module.weight.data = simulated_quantize_weight(\n",
        "        router_module.weight.data, bits\n",
        "    )\n",
        "    return backup\n",
        "\n",
        "def restore_router_safe(router_module, backup):\n",
        "    router_module.weight.data = backup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WDUwoADRrRa"
      },
      "outputs": [],
      "source": [
        "model.config.output_router_logits = True\n",
        "\n",
        "def get_router_logits(model, prompt, max_length=24):\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "    enc = enc.to(next(model.parameters()).device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(**enc, output_router_logits=True)\n",
        "\n",
        "    return out.router_logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RixUmGuyRssU"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def routing_entropy(logits):\n",
        "    p = F.softmax(logits, dim=-1)\n",
        "    return -(p * torch.log(p + 1e-9)).sum(dim=-1).mean().item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2wy2_NlUsFo"
      },
      "outputs": [],
      "source": [
        "def get_module_by_name(model, name):\n",
        "    module = model\n",
        "    for attr in name.split(\".\"):\n",
        "        module = getattr(module, attr)\n",
        "    return module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7nJ5mqwWRHu"
      },
      "outputs": [],
      "source": [
        "def simulated_quantize_weight(w, bits=2):\n",
        "    qmin = 0\n",
        "    qmax = (1 << bits) - 1\n",
        "\n",
        "    min_w = w.min()\n",
        "    max_w = w.max()\n",
        "\n",
        "    if (max_w - min_w) < 1e-8:\n",
        "        return w.clone()\n",
        "\n",
        "    scale = (max_w - min_w) / qmax\n",
        "    q = torch.round((w - min_w) / scale)\n",
        "    q = torch.clamp(q, qmin, qmax)\n",
        "\n",
        "    return q * scale + min_w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS1RzxoZWSjE"
      },
      "outputs": [],
      "source": [
        "def quantize_router_safe(router_module, bits=2):\n",
        "    backup = router_module.weight.data.clone()\n",
        "    router_module.weight.data = simulated_quantize_weight(\n",
        "        router_module.weight.data, bits\n",
        "    )\n",
        "    return backup\n",
        "\n",
        "\n",
        "def restore_router_safe(router_module, backup):\n",
        "    router_module.weight.data = backup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sumipH4WUJn"
      },
      "outputs": [],
      "source": [
        "def get_router_logits(model, prompt, max_length=24):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    enc = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(**enc, output_router_logits=True)\n",
        "\n",
        "    return out.router_logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTeyi4F9WVZj"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def routing_entropy(logits):\n",
        "    p = F.softmax(logits, dim=-1)\n",
        "    return -(p * torch.log(p + 1e-9)).sum(dim=-1).mean().item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F4sVgJPWW3Y"
      },
      "outputs": [],
      "source": [
        "model.config.use_cache = False\n",
        "model.config.output_router_logits = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J_6Va6FWard"
      },
      "outputs": [],
      "source": [
        "def is_meta_tensor(t):\n",
        "    return getattr(t, \"is_meta\", False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R_cD43AaIPK",
        "outputId": "529f4c0d-4dbe-4f29-eaf7-d6ce5f1dec48"
      },
      "outputs": [],
      "source": [
        "router_inputs = {}\n",
        "\n",
        "def capture_router_inputs(module, inp, out, name):\n",
        "    # inp[0] is the hidden state entering the router\n",
        "    router_inputs[name] = inp[0].detach().cpu()\n",
        "\n",
        "hooks = []\n",
        "\n",
        "for name in router_layers:\n",
        "    router = get_module_by_name(model, name)\n",
        "    hooks.append(\n",
        "        router.register_forward_hook(\n",
        "            lambda m, i, o, n=name: capture_router_inputs(m, i, o, n)\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Single forward pass\n",
        "_ = model(**tokenizer(\"The meaning of life is\", return_tensors=\"pt\").to(model.device))\n",
        "\n",
        "# Remove hooks immediately\n",
        "for h in hooks:\n",
        "    h.remove()\n",
        "\n",
        "print(\"Captured router inputs:\", len(router_inputs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovA6pmK1IMAA"
      },
      "outputs": [],
      "source": [
        "def simulated_quantize_weight(W, bits=2):\n",
        "    qmin = -(2 ** (bits - 1))\n",
        "    qmax = (2 ** (bits - 1)) - 1\n",
        "    scale = W.abs().max() / qmax + 1e-8\n",
        "    return (W / scale).round().clamp(qmin, qmax) * scale\n",
        "\n",
        "def router_logits(router, hidden):\n",
        "    return hidden @ router.weight.T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtYadOngIcPa",
        "outputId": "a2566097-3f7c-4ba1-da6d-f1c3b146d510"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for name in list(router_inputs.keys())[:6]:\n",
        "    router = get_module_by_name(model, name)\n",
        "    hidden = router_inputs[name]\n",
        "\n",
        "    if hidden.is_meta:\n",
        "        continue\n",
        "\n",
        "    # Baseline routing\n",
        "    base_logits = hidden @ router.weight.T\n",
        "    base_top = base_logits.argmax(dim=-1)\n",
        "    base_ent = routing_entropy(base_logits)\n",
        "\n",
        "    # Quantizing router weights\n",
        "    Wq = simulated_quantize_weight(router.weight.data, bits=2)\n",
        "\n",
        "    quant_logits = hidden @ Wq.T\n",
        "    q_top = quant_logits.argmax(dim=-1)\n",
        "    q_ent = routing_entropy(quant_logits)\n",
        "\n",
        "    flip_rate = (base_top != q_top).float().mean().item()\n",
        "\n",
        "    results.append({\n",
        "        \"layer\": name,\n",
        "        \"flip_rate\": flip_rate,\n",
        "        \"entropy_drop\": q_ent - base_ent\n",
        "    })\n",
        "\n",
        "    print(\n",
        "        name,\n",
        "        \"| flip_rate:\", round(flip_rate, 3),\n",
        "        \"| Δentropy:\", round(q_ent - base_ent, 3)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7oi1h60IeFY",
        "outputId": "bcbd023c-4d5b-47d3-9d7b-97fa187fe126"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for name in list(router_inputs.keys())[:6]:\n",
        "    router = get_module_by_name(model, name)\n",
        "    hidden = router_inputs[name]\n",
        "\n",
        "    if hidden.is_meta:\n",
        "        continue\n",
        "\n",
        "    # Baseline\n",
        "    base_logits = hidden @ router.weight.T\n",
        "    base_top = base_logits.argmax(dim=-1)\n",
        "    base_ent = routing_entropy(base_logits)\n",
        "\n",
        "    # for 2-bit\n",
        "    Wq2 = simulated_quantize_weight(router.weight.data, bits=2)\n",
        "    q2_logits = hidden @ Wq2.T\n",
        "    q2_top = q2_logits.argmax(dim=-1)\n",
        "    ent_2 = routing_entropy(q2_logits)\n",
        "    flip_2 = (base_top != q2_top).float().mean().item()\n",
        "\n",
        "    # for 4-bit -\n",
        "    Wq4 = simulated_quantize_weight(router.weight.data, bits=4)\n",
        "    q4_logits = hidden @ Wq4.T\n",
        "    q4_top = q4_logits.argmax(dim=-1)\n",
        "    ent_4 = routing_entropy(q4_logits)\n",
        "    flip_4 = (base_top != q4_top).float().mean().item()\n",
        "\n",
        "    results.append({\n",
        "        \"layer\": name,\n",
        "        \"flip_2b\": flip_2,\n",
        "        \"flip_4b\": flip_4,\n",
        "        \"Δentropy_2b\": ent_2 - base_ent,\n",
        "        \"Δentropy_4b\": ent_4 - base_ent\n",
        "    })\n",
        "\n",
        "    print(\n",
        "        name,\n",
        "        \"| flip@2b:\", round(flip_2, 2),\n",
        "        \"| ΔH@2b:\", round(ent_2 - base_ent, 3),\n",
        "        \"| flip@4b:\", round(flip_4, 2),\n",
        "        \"| ΔH@4b:\", round(ent_4 - base_ent, 3),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX6Wjqe4KqUi",
        "outputId": "9fbb2705-a7ff-4e90-e268-4d2993bec697"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "# 3 random samples with seed 42\n",
        "sample_experts = random.sample(expert_layers, 3)\n",
        "print(sample_experts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h4GpEcCMCaV",
        "outputId": "c4430bfe-fd42-4b3e-fb56-878cff011bd5"
      },
      "outputs": [],
      "source": [
        "expert_results = []\n",
        "\n",
        "for expert_name in sample_experts:\n",
        "    expert = get_module_by_name(model, expert_name)\n",
        "\n",
        "    # skipping meta\n",
        "    if expert.weight.is_meta:\n",
        "        print(expert_name, \"| skipped (meta)\")\n",
        "        continue\n",
        "\n",
        "    # baseline routing\n",
        "    base_logits = router_inputs[list(router_inputs.keys())[0]] @ \\\n",
        "                  get_module_by_name(model, router_layers[0]).weight.T\n",
        "    base_top = base_logits.argmax(dim=-1)\n",
        "\n",
        "    # quantizing expert weights\n",
        "    Wq = simulated_quantize_weight(expert.weight.data, bits=2)\n",
        "    backup = expert.weight.data.clone()\n",
        "    expert.weight.data = Wq\n",
        "\n",
        "    # routing after expert quantization\n",
        "    quant_logits = router_inputs[list(router_inputs.keys())[0]] @ \\\n",
        "                   get_module_by_name(model, router_layers[0]).weight.T\n",
        "    quant_top = quant_logits.argmax(dim=-1)\n",
        "\n",
        "    # restoring expert\n",
        "    expert.weight.data = backup\n",
        "\n",
        "    flip_rate = (base_top != quant_top).float().mean().item()\n",
        "\n",
        "    expert_results.append({\n",
        "        \"expert\": expert_name,\n",
        "        \"flip_rate\": flip_rate\n",
        "    })\n",
        "\n",
        "    print(expert_name, \"| routing flip rate:\", flip_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNGhHiynMEhE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMfqkDdmvIAbw96YCuf+zan",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "063e002b589b4753a873363c467c9926": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ff327cce29a462da90d2e70b6672a8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "172f75c7b0d24dde84e32be7cf19737c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d3bbde80a794a629dd775327e795d04",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_063e002b589b4753a873363c467c9926",
            "value": 8
          }
        },
        "5d3bbde80a794a629dd775327e795d04": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "642cd914a60b41748e8ea5567ce29df1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c40d3d157440473aa5d93653e58e78b5",
            "placeholder": "​",
            "style": "IPY_MODEL_848879175aff4856ae72f4a6308a5bc2",
            "value": " 8/8 [01:03&lt;00:00, 21.16s/it]"
          }
        },
        "848879175aff4856ae72f4a6308a5bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9604bda4b44a458cb6453218204e2ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7bd2918930a475491f33de47d18d883",
              "IPY_MODEL_172f75c7b0d24dde84e32be7cf19737c",
              "IPY_MODEL_642cd914a60b41748e8ea5567ce29df1"
            ],
            "layout": "IPY_MODEL_dbb79d5f31b94412a67d7d6331b00dcd"
          }
        },
        "c40d3d157440473aa5d93653e58e78b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7bd2918930a475491f33de47d18d883": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ff327cce29a462da90d2e70b6672a8c",
            "placeholder": "​",
            "style": "IPY_MODEL_c9f84a95ceae45ec8d6e628f6ce0f007",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c9f84a95ceae45ec8d6e628f6ce0f007": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbb79d5f31b94412a67d7d6331b00dcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
